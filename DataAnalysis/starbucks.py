# -*- coding: utf-8 -*-
"""Starcucks

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18CImSRSFwNhrPgry_TOuvjBe_On_MgU3

Portfolio Exercise: Starbucks



Background Information
The dataset you will be provided in this portfolio exercise was originally used as a take-home assignment provided by Starbucks for their job candidates. The data for this exercise consists of about 120,000 data points split in a 2:1 ratio among training and test files. In the experiment simulated by the data, an advertising promotion was tested to see if it would bring more customers to purchase a specific product priced at $10. Since it costs the company 0.15 to send out each promotion, it would be best to limit that promotion only to those that are most receptive to the promotion. Each data point includes one column indicating whether or not an individual was sent a promotion for the product, and one column indicating whether or not that individual eventually purchased that product. Each individual also has seven additional features associated with them, which are provided abstractly as V1-V7.

Optimization Strategy
Your task is to use the training data to understand what patterns in V1-V7 to indicate that a promotion should be provided to a user. Specifically, your goal is to maximize the following metrics:

Incremental Response Rate (IRR)
IRR depicts how many more customers purchased the product with the promotion, as compared to if they didn't receive the promotion. Mathematically, it's the ratio of the number of purchasers in the promotion group to the total number of customers in the purchasers group (treatment) minus the ratio of the number of purchasers in the non-promotional group to the total number of customers in the non-promotional group (control).

ð¼ð‘…ð‘…=ð‘ð‘¢ð‘Ÿð‘â„Žð‘¡ð‘Ÿð‘’ð‘Žð‘¡ð‘ð‘¢ð‘ ð‘¡ð‘¡ð‘Ÿð‘’ð‘Žð‘¡âˆ’ð‘ð‘¢ð‘Ÿð‘â„Žð‘ð‘¡ð‘Ÿð‘™ð‘ð‘¢ð‘ ð‘¡ð‘ð‘¡ð‘Ÿð‘™
 
Net Incremental Revenue (NIR)
NIR depicts how much is made (or lost) by sending out the promotion. Mathematically, this is 10 times the total number of purchasers that received the promotion minus 0.15 times the number of promotions sent out, minus 10 times the number of purchasers who were not given the promotion.

ð‘ð¼ð‘…=(10â‹…ð‘ð‘¢ð‘Ÿð‘â„Žð‘¡ð‘Ÿð‘’ð‘Žð‘¡âˆ’0.15â‹…ð‘ð‘¢ð‘ ð‘¡ð‘¡ð‘Ÿð‘’ð‘Žð‘¡)âˆ’10â‹…ð‘ð‘¢ð‘Ÿð‘â„Žð‘ð‘¡ð‘Ÿð‘™
 
For a full description of what Starbucks provides to candidates see the instructions available here.

Below you can find the training data provided. Explore the data and different optimization strategies.

How To Test Your Strategy?
When you feel like you have an optimization strategy, complete the promotion_strategy function to pass to the test_results function.
From past data, we know there are four possible outomes:

Table of actual promotion vs. predicted promotion customers:

Actual
Predicted	Yes	No
Yes	I	II
No	III	IV
The metrics are only being compared for the individuals we predict should obtain the promotion â€“ that is, quadrants I and II. Since the first set of individuals that receive the promotion (in the training set) receive it randomly, we can expect that quadrants I and II will have approximately equivalent participants.

Comparing quadrant I to II then gives an idea of how well your promotion strategy will work in the future.

Get started by reading in the data below. See how each variable or combination of variables along with a promotion influences the chance of purchasing. When you feel like you have a strategy for who should receive a promotion, test your strategy against the test dataset used in the final test_results function.
"""

# Commented out IPython magic to ensure Python compatibility.
# load in packages

from itertools import combinations

import scipy as sp
import pandas as pd
import random

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# load in the data
train_data = pd.read_csv('../input/training.csv')
train_data.head()

import numpy as np
from time import time

# set random state
seed = 69 # Bill & Ted > Hitchhikers
np.random.seed(seed)
random.seed = seed

data = train_data.copy()

# find possibile categorical columns
cat_cols = []
for col in data.columns :
    if data[col].nunique() < 10 :
        print(col)
        print(data[col].unique())
        cat_cols.append(col)
print('\n')

# change promotion to dummy
data['Promotion'] = data['Promotion'].apply(
    lambda x : str(x) == 'Yes'
).astype(int)

# change V4 and V7 to be dummies, since they only have 2 unique values: 1 and 2
to_bool_cols = ['V4','V7']
for col in to_bool_cols :
    data[col] = data[col].apply(
        lambda x: x == 2
    ).astype(int)

print(f'number of data points: {data.shape[0]}')
print(f'Percent of customers who got coupons: {100 * data["Promotion"].sum()/data.shape[0]}')
print(f'Percent of customers who are purchasers: {100 * data["purchase"].sum()/data.shape[0]}')

# some of our variables may be categorical, so let's also make a dummy df
def get_dummies(df:pd.DataFrame, cols=None) :
    if cols is None:
        cols = ['V1', 'V5', 'V6']
    new_df = df.drop(columns=cols)
    for col in cols :
        new_df = pd.concat([new_df,pd.get_dummies(df[col],drop_first=True,prefix=col)],axis=1)
    new_df = new_df.reindex(sorted(new_df.columns), axis=1)
    return new_df

# remove ID for now because it's doubtful that it is meaningful
df = data[['V1','V2','V3','V4','V5','V6','V7']]
dummy_df = get_dummies(df, cols=None)

# define our scoring metric
from sklearn.metrics import make_scorer
def IRR(model,X,y_true) :
    y_pred = model.predict(X)
    purch_treat = np.sum((y_true == 1) & (y_pred == 1))
    cust_treat = np.sum(y_pred)
    irr = purch_treat / cust_treat
    purch_ctrl = np.sum((y_true == 1) & (y_pred == 0))
    cust_ctrl = np.sum(y_pred == 0)
    irr -= (purch_ctrl / cust_ctrl)
    return irr

def NIR(y_true,y_pred) :
    purch_treat = np.sum((y_true == 1) & (y_pred == 1))
    cust_treat = np.sum(y_pred == 1)
    purch_ctrl = np.sum(y_pred == 0)
    ctrl_purch = np.sum((y_true == 1) & (y_pred==0))
    return (10 * purch_treat - .15 * cust_treat) - (10 * ctrl_purch)

irr_score = make_scorer(IRR,greater_is_better=True)
nir_score = make_scorer(NIR,greater_is_better=True)

promo_df = pd.concat([dummy_df,data['Promotion'],data['purchase']],axis=1)
promo_df = promo_df[promo_df['Promotion']==1]
promo_promotion = promo_df.pop('Promotion')
promo_purchase = promo_df.pop('purchase')

# import model selection functions
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score,cross_val_predict
from sklearn.metrics import f1_score,accuracy_score
X_train,X_test,y_train,y_test = train_test_split(
    promo_df,promo_purchase,test_size=.2,shuffle=True)

# import oversampling technique
from imblearn.over_sampling import SMOTE

def oversample_dummy(input,output) :
    oversample_smote = SMOTE(sampling_strategy=1,random_state=seed)
    X = input.copy()
    y = output.copy()
    X,y = oversample_smote.fit_resample(X,y)
    X = pd.DataFrame(X,columns=dummy_df.columns)
    return X,y

X_train, y_train = oversample_dummy(X_train,y_train)

# verify that oversampling was successful
print(X_train.shape,y_train.shape)
y_train.sum() / y_train.shape[0]

# let's run some naiive regressions
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier

logit = LogisticRegression(max_iter=10000)
bayesian = GaussianNB()
lightgbm = LGBMClassifier()

ddd = {
    'logit':logit,
    'bayesian':bayesian,
    'lightgbm':lightgbm,
    'xgb': XGBClassifier()
}

# import some metrics
from sklearn.metrics import confusion_matrix

def evaluate(ddd=ddd,X=X_train,y=y_train) :
    cv = KFold(n_splits=4)
    for name,model in ddd.items() :
        t = time()
        score = cross_val_score(model,X,y,scoring=nir_score)
        print('{} score: {:.4f}, ({:.4f})'.format(name, score.mean(),score.std()))
        print('    time taken: {:.2f}s'.format(time()-t))
        model.fit(X_train,y_train)
        y_pred = cross_val_predict(model,X,y)
        print(confusion_matrix(y_train,y_pred))
        print('    train score: {:.2f}'.format(NIR(y_train,y_pred)))
        y_pred = model.predict(X_test)
        print(f'{confusion_matrix(y_test,y_pred)}')
        print('    test score: {:.2f}'.format(NIR(y_test,y_pred)))
        print()
    return score

#score = evaluate(ddd,X_train,y_train)

# lightgbm is by far the best performer
 # let's put together a full oversampled dataset

# lightgbm is by far the best performer
 # let's put together a full oversampled dataset
inp,output = oversample_dummy(dummy_df,data['purchase'])
best_model = lightgbm.fit(inp,output)

models= {
    'best':best_model
}
#evaluate(models)

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def gridsearch(model, parameters) :
  pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', model)
  ])

  clf = GridSearchCV(pipe,param_grid=parameters,scoring=nir_score,verbose=0)
  return clf

weightscale = 98.8/1.2

params = {
    'clf__booster':['dart'], # dart is best so far
    'clf__max_bin':[256,128], # 256 > 512
    'clf__n_jobs':[-2],
    'clf__random_state':[seed],
    'clf__n_estimators':[200,300], # 200 > 100
    'clf__learning_rate':[.025],
    'clf__reg_lambda':[1.5,1.7], # 1.5 > 1
    'clf__max_depth':[8,9,10], # 8 > 3
    'clf__colsample_bytree':[.8,.7,.6], # .8 > .9,1
    'clf__scale_pos_weight':[1,weightscale]
}
t = time()
print('\nFitting xgboost...')
xgb = XGBClassifier()
clf = gridsearch(xgb,parameters=params)

clf.fit(X_train,y_train)
print('\n\n----------------')
print(clf.best_params_)

y_pred = clf.predict(X_test)
print(f'\nNIR score: {NIR(y_test,y_pred)}')
print('Fitted in {:.1f}s'.format(time()-t))
print(f'\n{y_test.sum()}')
print(y_pred.sum())
import joblib
filename = 'finalized_clf_dummies.sav'
joblib.dump(clf.best_estimator_, filename)


#{'clf__booster': 'dart',
# 'clf__colsample_bytree': 0.8,
# 'clf__learning_rate': 0.025,
# 'clf__max_bin': 256,
# 'clf__max_depth': 8,
# 'clf__n_estimators': 200,
# 'clf__n_jobs': -2,
# 'clf__random_state': 69,
# 'clf__reg_lambda': 1.5,
# 'clf__scale_pos_weight': 82.33333333333333}
